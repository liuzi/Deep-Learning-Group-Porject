{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"./dog_breeds/all/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport shutil\nfrom glob import glob \n\nfrom sklearn.utils import shuffle \nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\n## Specify a GPU\nimport os","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '../input/'\nmy_dir = '../input/dog_breeds/'\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\n\n# train_folder = os.path.join(base_dir,'base_dir/train')\n# valid_folder = os.path.join(base_dir,'base_dir/valid')\n# test_folder = os.path.join(base_dir,'base_dir/test')\nval_file, train_file, test_file= [os.path.join(\"../input/dog-breeds\", name) \n                                          for name in os.listdir(\"../input/dog-breeds/\")]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_file)\ndf_val = pd.read_csv(val_file)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(test_file)\n# df_data = df.merge(labels, on = \"id\")\ndf_test.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"    breed_id                     breed                   id\n0  n02091032         Italian_greyhound  n02091032_10352.jpg\n1  n02099849  Chesapeake_Bay_retriever   n02099849_1997.jpg\n2  n02100583                    vizsla  n02100583_12639.jpg\n3  n02112137                      chow   n02112137_2664.jpg\n4  n02105855         Shetland_sheepdog  n02105855_11876.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>breed_id</th>\n      <th>breed</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n02091032</td>\n      <td>Italian_greyhound</td>\n      <td>n02091032_10352.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n02099849</td>\n      <td>Chesapeake_Bay_retriever</td>\n      <td>n02099849_1997.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n02100583</td>\n      <td>vizsla</td>\n      <td>n02100583_12639.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n02112137</td>\n      <td>chow</td>\n      <td>n02112137_2664.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n02105855</td>\n      <td>Shetland_sheepdog</td>\n      <td>n02105855_11876.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Only run for the first time: split training data into training and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(os.path.join(my_dir, \"all/\"))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = df_train['breed']\n\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\nfor fold in [train_folder, valid_folder, test_folder]:\n    for subf in train_y.unique():\n        os.makedirs(os.path.join(fold, subf))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_data_dir =  os.path.join(my_dir, \"all/\")\ndf_train.set_index('id', inplace=True)\ndf_val.set_index('id', inplace=True)\ndf_test.set_index('id', inplace=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_path = '../input/stanford-dogs-dataset/images/Images/'\nfor breed in os.listdir(origin_path):\n    for file in glob(os.path.join(origin_path,breed,'*.jpg')):\n        shutil.copyfile(file, os.path.join(origin_data_dir, file.split('/')[-1:][0]))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_train.index.values:\n    breed = str(df_train.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(train_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_test.index.values:\n    breed = str(df_test.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(test_folder, breed, image)\n    shutil.copyfile(src, dst)\nfor image in df_val.index.values:\n    breed = str(df_val.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(valid_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 299\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\nnum_test_samples = len(df_test)\n\ntrain_batch_size = 32\nval_batch_size = 32\ntest_batch_size = 32\nprint(\"Num of train samples: %d\" % num_train_samples)\nprint(\"Num of validation samples: %d\" % num_val_samples)\nprint(\"Num of test samples: %d\" % num_test_samples)\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\ntest_steps = np.ceil(num_test_samples / test_batch_size)\nprint(\"train_steps: %d\" % train_steps)\nprint(\"val_steps: %d\" % val_steps)\nprint(\"test_steps: %d\" % test_steps)","execution_count":18,"outputs":[{"output_type":"stream","text":"Num of train samples: 13171\nNum of validation samples: 3293\nNum of test samples: 4116\ntrain_steps: 412\nval_steps: 103\ntest_steps: 129\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n## Noramlize RGB values\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    # preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n    horizontal_flip=True,\n    vertical_flip=True)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = datagen.flow_from_directory(train_folder,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_folder,\n                                      target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                      batch_size=val_batch_size,\n                                      class_mode='categorical')\n\ntest_gen = datagen.flow_from_directory(test_folder,\n                                       target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                       batch_size=test_batch_size,\n                                       class_mode='categorical',\n                                       shuffle=False)","execution_count":20,"outputs":[{"output_type":"stream","text":"Found 13171 images belonging to 120 classes.\nFound 3293 images belonging to 120 classes.\nFound 4116 images belonging to 120 classes.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## InceptionResNetV2 Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Activation, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\n\nconv_base = InceptionResNetV2(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\n##version 1\n# model = Sequential()\n# model.add(conv_base)\n# model.add(GlobalAveragePooling2D())\n# model.add(layers.Dense(512, activation = \"relu\"))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n##version 2\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(GlobalMaxPooling2D())\n# #model.add(layers.Dense(120, activation = \"sigmoid\"))\n# model.add(layers.Dense(512, activation = \"relu\"))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"inception_resnet_v2_relu_softmax_10.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=20,\n                              callbacks=[reducel, checkpoint])","execution_count":22,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n412/412 [==============================] - 401s 974ms/step - loss: 1.4593 - acc: 0.6381 - val_loss: 1.0194 - val_acc: 0.7173\n\nEpoch 00001: val_acc improved from -inf to 0.71728, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 2/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.6338 - acc: 0.8072 - val_loss: 0.9449 - val_acc: 0.7349\n\nEpoch 00002: val_acc improved from 0.71728 to 0.73489, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 3/20\n412/412 [==============================] - 322s 783ms/step - loss: 0.4119 - acc: 0.8733 - val_loss: 0.9329 - val_acc: 0.7480\n\nEpoch 00003: val_acc improved from 0.73489 to 0.74795, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 4/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.2686 - acc: 0.9168 - val_loss: 0.9125 - val_acc: 0.7540\n\nEpoch 00004: val_acc improved from 0.74795 to 0.75402, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 5/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.2009 - acc: 0.9375 - val_loss: 0.9313 - val_acc: 0.7549\n\nEpoch 00005: val_acc improved from 0.75402 to 0.75493, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 6/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.1570 - acc: 0.9504 - val_loss: 0.9684 - val_acc: 0.7580\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00006: val_acc improved from 0.75493 to 0.75797, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 7/20\n412/412 [==============================] - 321s 779ms/step - loss: 0.0738 - acc: 0.9801 - val_loss: 0.7774 - val_acc: 0.7968\n\nEpoch 00007: val_acc improved from 0.75797 to 0.79684, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 8/20\n412/412 [==============================] - 322s 782ms/step - loss: 0.0466 - acc: 0.9877 - val_loss: 0.7792 - val_acc: 0.8029\n\nEpoch 00008: val_acc improved from 0.79684 to 0.80292, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 9/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.0327 - acc: 0.9918 - val_loss: 0.8035 - val_acc: 0.8041\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00009: val_acc improved from 0.80292 to 0.80413, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 10/20\n412/412 [==============================] - 322s 782ms/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.7590 - val_acc: 0.8154\n\nEpoch 00010: val_acc improved from 0.80413 to 0.81537, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 11/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.0169 - acc: 0.9964 - val_loss: 0.7632 - val_acc: 0.8187\n\nEpoch 00011: val_acc improved from 0.81537 to 0.81871, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 12/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.7705 - val_acc: 0.8163\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00012: val_acc did not improve from 0.81871\nEpoch 13/20\n412/412 [==============================] - 321s 780ms/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.7751 - val_acc: 0.8196\n\nEpoch 00013: val_acc improved from 0.81871 to 0.81962, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 14/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.7712 - val_acc: 0.8181\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00014: val_acc did not improve from 0.81962\nEpoch 15/20\n412/412 [==============================] - 321s 779ms/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.7957 - val_acc: 0.8190\n\nEpoch 00015: val_acc did not improve from 0.81962\nEpoch 16/20\n412/412 [==============================] - 321s 780ms/step - loss: 0.0066 - acc: 0.9983 - val_loss: 0.7995 - val_acc: 0.8199\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\nEpoch 00016: val_acc improved from 0.81962 to 0.81992, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 17/20\n412/412 [==============================] - 323s 783ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.7821 - val_acc: 0.8224\n\nEpoch 00017: val_acc improved from 0.81992 to 0.82235, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 18/20\n412/412 [==============================] - 323s 783ms/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.8109 - val_acc: 0.8154\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\nEpoch 00018: val_acc did not improve from 0.82235\nEpoch 19/20\n412/412 [==============================] - 321s 780ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.7665 - val_acc: 0.8187\n\nEpoch 00019: val_acc did not improve from 0.82235\nEpoch 20/20\n412/412 [==============================] - 322s 781ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.7880 - val_acc: 0.8120\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n\nEpoch 00020: val_acc did not improve from 0.82235\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":23,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[0.7806696782161937, 0.8206997085127353]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}