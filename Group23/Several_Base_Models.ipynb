{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"./dog_breeds/all/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport shutil\nfrom glob import glob \n\nfrom sklearn.utils import shuffle \nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\n## Specify a GPU\nimport os","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '../input/'\nmy_dir = '../input/dog_breeds/'\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\n\n# train_folder = os.path.join(base_dir,'base_dir/train')\n# valid_folder = os.path.join(base_dir,'base_dir/valid')\n# test_folder = os.path.join(base_dir,'base_dir/test')\nval_file, train_file, test_file= [os.path.join(\"../input/dog-breeds\", name) \n                                          for name in os.listdir(\"../input/dog-breeds/\")]","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_file)\ndf_val = pd.read_csv(val_file)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(test_file)\n# df_data = df.merge(labels, on = \"id\")\ndf_test.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"    breed_id         ...                            id\n0  n02094114         ...            n02094114_2449.jpg\n1  n02106030         ...           n02106030_16181.jpg\n2  n02089078         ...            n02089078_4497.jpg\n3  n02098286         ...            n02098286_2863.jpg\n4  n02105505         ...            n02105505_3030.jpg\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>breed_id</th>\n      <th>breed</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n02094114</td>\n      <td>Norfolk_terrier</td>\n      <td>n02094114_2449.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n02106030</td>\n      <td>collie</td>\n      <td>n02106030_16181.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n02089078</td>\n      <td>black</td>\n      <td>n02089078_4497.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n02098286</td>\n      <td>West_Highland_white_terrier</td>\n      <td>n02098286_2863.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n02105505</td>\n      <td>komondor</td>\n      <td>n02105505_3030.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Only run for the first time: split training data into training and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(os.path.join(my_dir, \"all/\"))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = df_train['breed']\n\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\nfor fold in [train_folder, valid_folder, test_folder]:\n    for subf in train_y.unique():\n        os.makedirs(os.path.join(fold, subf))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_data_dir =  os.path.join(my_dir, \"all/\")\ndf_train.set_index('id', inplace=True)\ndf_val.set_index('id', inplace=True)\ndf_test.set_index('id', inplace=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_path = '../input/stanford-dogs-dataset/images/Images/'\nfor breed in os.listdir(origin_path):\n    for file in glob(os.path.join(origin_path,breed,'*.jpg')):\n        shutil.copyfile(file, os.path.join(origin_data_dir, file.split('/')[-1:][0]))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_train.index.values:\n    breed = str(df_train.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(train_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_test.index.values:\n    breed = str(df_test.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(test_folder, breed, image)\n    shutil.copyfile(src, dst)\nfor image in df_val.index.values:\n    breed = str(df_val.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(valid_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 200\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\nnum_test_samples = len(df_test)\n\ntrain_batch_size = 32\nval_batch_size = 32\ntest_batch_size = 32\nprint(\"Num of train samples: %d\" % num_train_samples)\nprint(\"Num of validation samples: %d\" % num_val_samples)\nprint(\"Num of test samples: %d\" % num_test_samples)\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\ntest_steps = np.ceil(num_test_samples / test_batch_size)\nprint(\"train_steps: %d\" % train_steps)\nprint(\"val_steps: %d\" % val_steps)\nprint(\"test_steps: %d\" % test_steps)","execution_count":31,"outputs":[{"output_type":"stream","text":"Num of train samples: 4116\nNum of validation samples: 3293\nNum of test samples: 13171\ntrain_steps: 129\nval_steps: 103\ntest_steps: 412\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n## Noramlize RGB values\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    # preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n    horizontal_flip=True,\n    vertical_flip=True)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = datagen.flow_from_directory(train_folder,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_folder,\n                                      target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                      batch_size=val_batch_size,\n                                      class_mode='categorical')\n\ntest_gen = datagen.flow_from_directory(test_folder,\n                                       target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                       batch_size=test_batch_size,\n                                       class_mode='categorical',\n                                       shuffle=False)","execution_count":33,"outputs":[{"output_type":"stream","text":"Found 4116 images belonging to 120 classes.\nFound 3293 images belonging to 120 classes.\nFound 13171 images belonging to 120 classes.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## InceptionResNetV2 Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, Activation, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\ndropout_dense=0.5\n\nconv_base = InceptionResNetV2(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\n## Version1\n# model.add(GlobalAveragePooling2D())\n# model.add(layers.Dense(1024, activation = \"relu\"))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n##Version 2 \n# model.add(GlobalAveragePooling2D())\n# model.add(layers.Dense(1024))\n# model.add(BatchNormalization())\n# model.add(Activation(\"relu\"))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n\n## Version 3\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(GlobalMaxPooling2D())\n# #model.add(layers.Dense(120, activation = \"sigmoid\"))\n# model.add(layers.Dense(512, activation = \"relu\"))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n## Version 4\n#model.add(GlobalAveragePooling2D())\n#model.add(GlobalMaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"inception_resnet_v2_relu_softmax_10.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=25,\n                              callbacks=[reducel, checkpoint])","execution_count":28,"outputs":[{"output_type":"stream","text":"Epoch 1/25\n129/129 [==============================] - 232s 2s/step - loss: 3.3896 - acc: 0.3242 - val_loss: 2.0304 - val_acc: 0.6116\n\nEpoch 00001: val_acc improved from -inf to 0.61160, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 2/25\n129/129 [==============================] - 120s 926ms/step - loss: 1.7131 - acc: 0.6712 - val_loss: 1.6571 - val_acc: 0.6496\n\nEpoch 00002: val_acc improved from 0.61160 to 0.64956, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 3/25\n129/129 [==============================] - 119s 923ms/step - loss: 1.1017 - acc: 0.7843 - val_loss: 1.5193 - val_acc: 0.6881\n\nEpoch 00003: val_acc improved from 0.64956 to 0.68813, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 4/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.7819 - acc: 0.8542 - val_loss: 1.5125 - val_acc: 0.6760\n\nEpoch 00004: val_acc did not improve from 0.68813\nEpoch 5/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.5674 - acc: 0.8993 - val_loss: 1.5473 - val_acc: 0.6851\n\nEpoch 00005: val_acc did not improve from 0.68813\nEpoch 6/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.4050 - acc: 0.9320 - val_loss: 1.3980 - val_acc: 0.6978\n\nEpoch 00006: val_acc improved from 0.68813 to 0.69784, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 7/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.3611 - acc: 0.9355 - val_loss: 1.6813 - val_acc: 0.6735\n\nEpoch 00007: val_acc did not improve from 0.69784\nEpoch 8/25\n129/129 [==============================] - 119s 921ms/step - loss: 0.2546 - acc: 0.9530 - val_loss: 1.3830 - val_acc: 0.7021\n\nEpoch 00008: val_acc improved from 0.69784 to 0.70210, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 9/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.2246 - acc: 0.9612 - val_loss: 1.4073 - val_acc: 0.7012\n\nEpoch 00009: val_acc did not improve from 0.70210\nEpoch 10/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.1864 - acc: 0.9717 - val_loss: 1.3713 - val_acc: 0.6997\n\nEpoch 00010: val_acc did not improve from 0.70210\nEpoch 11/25\n129/129 [==============================] - 119s 921ms/step - loss: 0.1495 - acc: 0.9753 - val_loss: 1.4367 - val_acc: 0.6942\n\nEpoch 00011: val_acc did not improve from 0.70210\nEpoch 12/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.1419 - acc: 0.9770 - val_loss: 1.3873 - val_acc: 0.6942\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00012: val_acc did not improve from 0.70210\nEpoch 13/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.0981 - acc: 0.9864 - val_loss: 1.2111 - val_acc: 0.7303\n\nEpoch 00013: val_acc improved from 0.70210 to 0.73034, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 14/25\n129/129 [==============================] - 120s 929ms/step - loss: 0.0551 - acc: 0.9944 - val_loss: 1.1839 - val_acc: 0.7376\n\nEpoch 00014: val_acc improved from 0.73034 to 0.73763, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 15/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0516 - acc: 0.9947 - val_loss: 1.1981 - val_acc: 0.7425\n\nEpoch 00015: val_acc improved from 0.73763 to 0.74248, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 16/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0484 - acc: 0.9952 - val_loss: 1.2524 - val_acc: 0.7358\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00016: val_acc did not improve from 0.74248\nEpoch 17/25\n129/129 [==============================] - 119s 920ms/step - loss: 0.0400 - acc: 0.9957 - val_loss: 1.1632 - val_acc: 0.7446\n\nEpoch 00017: val_acc improved from 0.74248 to 0.74461, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 18/25\n129/129 [==============================] - 119s 926ms/step - loss: 0.0313 - acc: 0.9973 - val_loss: 1.1767 - val_acc: 0.7446\n\nEpoch 00018: val_acc did not improve from 0.74461\nEpoch 19/25\n129/129 [==============================] - 119s 920ms/step - loss: 0.0287 - acc: 0.9964 - val_loss: 1.1572 - val_acc: 0.7501\n\nEpoch 00019: val_acc improved from 0.74461 to 0.75008, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 20/25\n129/129 [==============================] - 119s 926ms/step - loss: 0.0251 - acc: 0.9976 - val_loss: 1.1504 - val_acc: 0.7510\n\nEpoch 00020: val_acc improved from 0.75008 to 0.75099, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 21/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.0248 - acc: 0.9978 - val_loss: 1.1759 - val_acc: 0.7528\n\nEpoch 00021: val_acc improved from 0.75099 to 0.75281, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 22/25\n129/129 [==============================] - 121s 937ms/step - loss: 0.0202 - acc: 0.9985 - val_loss: 1.1801 - val_acc: 0.7519\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00022: val_acc did not improve from 0.75281\nEpoch 23/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.0208 - acc: 0.9993 - val_loss: 1.1365 - val_acc: 0.7571\n\nEpoch 00023: val_acc improved from 0.75281 to 0.75706, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 24/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.0197 - acc: 0.9983 - val_loss: 1.1365 - val_acc: 0.7647\n\nEpoch 00024: val_acc improved from 0.75706 to 0.76465, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 25/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0188 - acc: 0.9986 - val_loss: 1.2029 - val_acc: 0.7461\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00025: val_acc did not improve from 0.76465\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":29,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.1206932214427652, 0.7601548857519109]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Xception Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Xception\nfrom keras.applications.xception import Xception\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = Xception(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\")\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"xception.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=20,\n                              callbacks=[reducel, checkpoint])","execution_count":19,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n129/129 [==============================] - 77s 599ms/step - loss: 1.9983 - acc: 0.5819 - val_loss: 1.5942 - val_acc: 0.5940\n\nEpoch 00001: val_acc improved from -inf to 0.59399, saving model to xception.h5\nEpoch 2/20\n129/129 [==============================] - 60s 468ms/step - loss: 0.4172 - acc: 0.9139 - val_loss: 1.4777 - val_acc: 0.6213\n\nEpoch 00002: val_acc improved from 0.59399 to 0.62132, saving model to xception.h5\nEpoch 3/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.2132 - acc: 0.9555 - val_loss: 1.4880 - val_acc: 0.6383\n\nEpoch 00003: val_acc improved from 0.62132 to 0.63832, saving model to xception.h5\nEpoch 4/20\n129/129 [==============================] - 61s 472ms/step - loss: 0.1370 - acc: 0.9734 - val_loss: 1.4914 - val_acc: 0.6408\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00004: val_acc improved from 0.63832 to 0.64075, saving model to xception.h5\nEpoch 5/20\n129/129 [==============================] - 61s 475ms/step - loss: 0.0971 - acc: 0.9838 - val_loss: 1.4381 - val_acc: 0.6408\n\nEpoch 00005: val_acc improved from 0.64075 to 0.64075, saving model to xception.h5\nEpoch 6/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0545 - acc: 0.9935 - val_loss: 1.4487 - val_acc: 0.6435\n\nEpoch 00006: val_acc improved from 0.64075 to 0.64349, saving model to xception.h5\nEpoch 7/20\n129/129 [==============================] - 61s 470ms/step - loss: 0.0447 - acc: 0.9947 - val_loss: 1.5057 - val_acc: 0.6359\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00007: val_acc did not improve from 0.64349\nEpoch 8/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0431 - acc: 0.9937 - val_loss: 1.4283 - val_acc: 0.6575\n\nEpoch 00008: val_acc improved from 0.64349 to 0.65746, saving model to xception.h5\nEpoch 9/20\n129/129 [==============================] - 61s 476ms/step - loss: 0.0317 - acc: 0.9964 - val_loss: 1.4342 - val_acc: 0.6508\n\nEpoch 00009: val_acc did not improve from 0.65746\nEpoch 10/20\n129/129 [==============================] - 61s 471ms/step - loss: 0.0294 - acc: 0.9965 - val_loss: 1.4855 - val_acc: 0.6471\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00010: val_acc did not improve from 0.65746\nEpoch 11/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0211 - acc: 0.9978 - val_loss: 1.4636 - val_acc: 0.6502\n\nEpoch 00011: val_acc did not improve from 0.65746\nEpoch 12/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0213 - acc: 0.9981 - val_loss: 1.4402 - val_acc: 0.6529\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00012: val_acc did not improve from 0.65746\nEpoch 13/20\n129/129 [==============================] - 61s 470ms/step - loss: 0.0197 - acc: 0.9981 - val_loss: 1.4362 - val_acc: 0.6490\n\nEpoch 00013: val_acc did not improve from 0.65746\nEpoch 14/20\n129/129 [==============================] - 62s 482ms/step - loss: 0.0179 - acc: 0.9985 - val_loss: 1.4435 - val_acc: 0.6505\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\nEpoch 00014: val_acc did not improve from 0.65746\nEpoch 15/20\n129/129 [==============================] - 60s 466ms/step - loss: 0.0171 - acc: 0.9988 - val_loss: 1.4784 - val_acc: 0.6444\n\nEpoch 00015: val_acc did not improve from 0.65746\nEpoch 16/20\n129/129 [==============================] - 60s 467ms/step - loss: 0.0157 - acc: 0.9993 - val_loss: 1.4471 - val_acc: 0.6468\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\nEpoch 00016: val_acc did not improve from 0.65746\nEpoch 17/20\n129/129 [==============================] - 60s 469ms/step - loss: 0.0157 - acc: 0.9988 - val_loss: 1.4730 - val_acc: 0.6496\n\nEpoch 00017: val_acc did not improve from 0.65746\nEpoch 18/20\n129/129 [==============================] - 61s 471ms/step - loss: 0.0158 - acc: 0.9993 - val_loss: 1.4442 - val_acc: 0.6572\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n\nEpoch 00018: val_acc did not improve from 0.65746\nEpoch 19/20\n129/129 [==============================] - 61s 474ms/step - loss: 0.0158 - acc: 0.9990 - val_loss: 1.4655 - val_acc: 0.6517\n\nEpoch 00019: val_acc did not improve from 0.65746\nEpoch 20/20\n129/129 [==============================] - 60s 467ms/step - loss: 0.0149 - acc: 0.9990 - val_loss: 1.4345 - val_acc: 0.6544\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n\nEpoch 00020: val_acc did not improve from 0.65746\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":20,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.3929535331183744, 0.6614531926608793]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## ResNet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"## ResNet50\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = ResNet50(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":43,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n","name":"stderr"},{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94658560/94653016 [==============================] - 3s 0us/step\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-9211be0ae5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIN_SHAPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 md5_hash='a268eb855778b3df3c7506639542a6af')\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mkeras_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                              ' elements.')\n\u001b[1;32m   1057\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"ResNet50.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=10,\n                              callbacks=[reducel, checkpoint])","execution_count":44,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n129/129 [==============================] - 112s 870ms/step - loss: 0.0923 - acc: 0.9816 - val_loss: 1.5923 - val_acc: 0.5946\n\nEpoch 00001: val_acc improved from -inf to 0.59459, saving model to ResNet50.h5\nEpoch 2/10\n129/129 [==============================] - 43s 330ms/step - loss: 0.0846 - acc: 0.9850 - val_loss: 1.6089 - val_acc: 0.5864\n\nEpoch 00002: val_acc did not improve from 0.59459\nEpoch 3/10\n129/129 [==============================] - 43s 330ms/step - loss: 0.0684 - acc: 0.9890 - val_loss: 1.6231 - val_acc: 0.5846\n\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00003: val_acc did not improve from 0.59459\nEpoch 4/10\n129/129 [==============================] - 42s 329ms/step - loss: 0.0562 - acc: 0.9923 - val_loss: 1.5766 - val_acc: 0.5961\n\nEpoch 00004: val_acc improved from 0.59459 to 0.59611, saving model to ResNet50.h5\nEpoch 5/10\n129/129 [==============================] - 43s 330ms/step - loss: 0.0500 - acc: 0.9932 - val_loss: 1.5909 - val_acc: 0.5888\n\nEpoch 00005: val_acc did not improve from 0.59611\nEpoch 6/10\n129/129 [==============================] - 41s 320ms/step - loss: 0.0518 - acc: 0.9913 - val_loss: 1.6087 - val_acc: 0.5925\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00006: val_acc did not improve from 0.59611\nEpoch 7/10\n129/129 [==============================] - 42s 326ms/step - loss: 0.0464 - acc: 0.9936 - val_loss: 1.6158 - val_acc: 0.5916\n\nEpoch 00007: val_acc did not improve from 0.59611\nEpoch 8/10\n129/129 [==============================] - 42s 327ms/step - loss: 0.0403 - acc: 0.9956 - val_loss: 1.5813 - val_acc: 0.5922\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\nEpoch 00008: val_acc did not improve from 0.59611\nEpoch 9/10\n129/129 [==============================] - 42s 324ms/step - loss: 0.0397 - acc: 0.9952 - val_loss: 1.6224 - val_acc: 0.5870\n\nEpoch 00009: val_acc did not improve from 0.59611\nEpoch 10/10\n129/129 [==============================] - 41s 318ms/step - loss: 0.0331 - acc: 0.9971 - val_loss: 1.6254 - val_acc: 0.5840\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\nEpoch 00010: val_acc did not improve from 0.59611\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":45,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.5431418095405423, 0.6085339002534673]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"## InceptionV3\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = InceptionV3(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":36,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87916544/87910968 [==============================] - 2s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"InceptionV3.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=10,\n                              callbacks=[reducel, checkpoint])","execution_count":37,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n129/129 [==============================] - 128s 989ms/step - loss: 3.9884 - acc: 0.1672 - val_loss: 2.8011 - val_acc: 0.3371\n\nEpoch 00001: val_acc improved from -inf to 0.33708, saving model to InceptionV3.h5\nEpoch 2/10\n129/129 [==============================] - 41s 314ms/step - loss: 2.1775 - acc: 0.4565 - val_loss: 2.0992 - val_acc: 0.4446\n\nEpoch 00002: val_acc improved from 0.33708 to 0.44458, saving model to InceptionV3.h5\nEpoch 3/10\n129/129 [==============================] - 41s 321ms/step - loss: 1.4280 - acc: 0.6071 - val_loss: 1.9280 - val_acc: 0.4719\n\nEpoch 00003: val_acc improved from 0.44458 to 0.47191, saving model to InceptionV3.h5\nEpoch 4/10\n129/129 [==============================] - 41s 318ms/step - loss: 0.9760 - acc: 0.7259 - val_loss: 1.7866 - val_acc: 0.5141\n\nEpoch 00004: val_acc improved from 0.47191 to 0.51412, saving model to InceptionV3.h5\nEpoch 5/10\n129/129 [==============================] - 43s 330ms/step - loss: 0.6936 - acc: 0.8045 - val_loss: 1.9628 - val_acc: 0.4974\n\nEpoch 00005: val_acc did not improve from 0.51412\nEpoch 6/10\n129/129 [==============================] - 41s 315ms/step - loss: 0.5228 - acc: 0.8518 - val_loss: 1.9430 - val_acc: 0.5080\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00006: val_acc did not improve from 0.51412\nEpoch 7/10\n129/129 [==============================] - 40s 314ms/step - loss: 0.3247 - acc: 0.9136 - val_loss: 1.6171 - val_acc: 0.5739\n\nEpoch 00007: val_acc improved from 0.51412 to 0.57394, saving model to InceptionV3.h5\nEpoch 8/10\n129/129 [==============================] - 42s 327ms/step - loss: 0.1992 - acc: 0.9548 - val_loss: 1.6619 - val_acc: 0.5627\n\nEpoch 00008: val_acc did not improve from 0.57394\nEpoch 9/10\n129/129 [==============================] - 40s 314ms/step - loss: 0.1665 - acc: 0.9602 - val_loss: 1.6466 - val_acc: 0.5736\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00009: val_acc did not improve from 0.57394\nEpoch 10/10\n129/129 [==============================] - 41s 318ms/step - loss: 0.1250 - acc: 0.9744 - val_loss: 1.6482 - val_acc: 0.5834\n\nEpoch 00010: val_acc improved from 0.57394 to 0.58336, saving model to InceptionV3.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":38,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.5274156829543486, 0.6010933110259785]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}