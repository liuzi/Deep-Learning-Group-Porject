{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"./dog_breeds/all/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport shutil\nfrom glob import glob \n\nfrom sklearn.utils import shuffle \nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\n## Specify a GPU\nimport os","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '../input/'\nmy_dir = '../input/dog_breeds/'\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\n\n# train_folder = os.path.join(base_dir,'base_dir/train')\n# valid_folder = os.path.join(base_dir,'base_dir/valid')\n# test_folder = os.path.join(base_dir,'base_dir/test')\nval_file, train_file, test_file= [os.path.join(\"../input/dog-breeds\", name) \n                                          for name in os.listdir(\"../input/dog-breeds/\")]","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_file)\ndf_val = pd.read_csv(val_file)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(test_file)\n# df_data = df.merge(labels, on = \"id\")\ndf_test.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"    breed_id         ...                            id\n0  n02094114         ...            n02094114_2449.jpg\n1  n02106030         ...           n02106030_16181.jpg\n2  n02089078         ...            n02089078_4497.jpg\n3  n02098286         ...            n02098286_2863.jpg\n4  n02105505         ...            n02105505_3030.jpg\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>breed_id</th>\n      <th>breed</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n02094114</td>\n      <td>Norfolk_terrier</td>\n      <td>n02094114_2449.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n02106030</td>\n      <td>collie</td>\n      <td>n02106030_16181.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n02089078</td>\n      <td>black</td>\n      <td>n02089078_4497.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n02098286</td>\n      <td>West_Highland_white_terrier</td>\n      <td>n02098286_2863.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n02105505</td>\n      <td>komondor</td>\n      <td>n02105505_3030.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Only run for the first time: split training data into training and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(os.path.join(my_dir, \"all/\"))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = df_train['breed']\n\ntrain_folder = os.path.join(my_dir,'train')\nvalid_folder = os.path.join(my_dir,'valid')\ntest_folder = os.path.join(my_dir,'test')\nfor fold in [train_folder, valid_folder, test_folder]:\n    for subf in train_y.unique():\n        os.makedirs(os.path.join(fold, subf))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_data_dir =  os.path.join(my_dir, \"all/\")\ndf_train.set_index('id', inplace=True)\ndf_val.set_index('id', inplace=True)\ndf_test.set_index('id', inplace=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_path = '../input/stanford-dogs-dataset/images/Images/'\nfor breed in os.listdir(origin_path):\n    for file in glob(os.path.join(origin_path,breed,'*.jpg')):\n        shutil.copyfile(file, os.path.join(origin_data_dir, file.split('/')[-1:][0]))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_train.index.values:\n    breed = str(df_train.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(train_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_test.index.values:\n    breed = str(df_test.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(test_folder, breed, image)\n    shutil.copyfile(src, dst)\nfor image in df_val.index.values:\n    breed = str(df_val.loc[image,'breed']) # get the label for a certain image\n    src = os.path.join(origin_data_dir, image)\n    dst = os.path.join(valid_folder, breed, image)\n    shutil.copyfile(src, dst)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 200\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\nnum_test_samples = len(df_test)\n\ntrain_batch_size = 32\nval_batch_size = 32\ntest_batch_size = 32\nprint(\"Num of train samples: %d\" % num_train_samples)\nprint(\"Num of validation samples: %d\" % num_val_samples)\nprint(\"Num of test samples: %d\" % num_test_samples)\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\ntest_steps = np.ceil(num_test_samples / test_batch_size)\nprint(\"train_steps: %d\" % train_steps)\nprint(\"val_steps: %d\" % val_steps)\nprint(\"test_steps: %d\" % test_steps)","execution_count":31,"outputs":[{"output_type":"stream","text":"Num of train samples: 4116\nNum of validation samples: 3293\nNum of test samples: 13171\ntrain_steps: 129\nval_steps: 103\ntest_steps: 412\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n## Noramlize RGB values\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    # preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n    horizontal_flip=True,\n    vertical_flip=True)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = datagen.flow_from_directory(train_folder,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_folder,\n                                      target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                      batch_size=val_batch_size,\n                                      class_mode='categorical')\n\ntest_gen = datagen.flow_from_directory(test_folder,\n                                       target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                       batch_size=test_batch_size,\n                                       class_mode='categorical',\n                                       shuffle=False)","execution_count":33,"outputs":[{"output_type":"stream","text":"Found 4116 images belonging to 120 classes.\nFound 3293 images belonging to 120 classes.\nFound 13171 images belonging to 120 classes.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## InceptionResNetV2 Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, Activation, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\ndropout_dense=0.5\n\nconv_base = InceptionResNetV2(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\n## Version1\n# model.add(GlobalAveragePooling2D())\n# model.add(layers.Dense(1024, activation = \"relu\"))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n##Version 2 \n# model.add(GlobalAveragePooling2D())\n# model.add(layers.Dense(1024))\n# model.add(BatchNormalization())\n# model.add(Activation(\"relu\"))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n\n## Version 3\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(GlobalMaxPooling2D())\n# #model.add(layers.Dense(120, activation = \"sigmoid\"))\n# model.add(layers.Dense(512, activation = \"relu\"))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.001))\n# model.add(layers.Dense(120, activation = \"softmax\"))\n\n## Version 4\n#model.add(GlobalAveragePooling2D())\n#model.add(GlobalMaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.001))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"inception_resnet_v2_relu_softmax_10.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=20,\n                              callbacks=[reducel, checkpoint])","execution_count":28,"outputs":[{"output_type":"stream","text":"Epoch 1/25\n129/129 [==============================] - 232s 2s/step - loss: 3.3896 - acc: 0.3242 - val_loss: 2.0304 - val_acc: 0.6116\n\nEpoch 00001: val_acc improved from -inf to 0.61160, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 2/25\n129/129 [==============================] - 120s 926ms/step - loss: 1.7131 - acc: 0.6712 - val_loss: 1.6571 - val_acc: 0.6496\n\nEpoch 00002: val_acc improved from 0.61160 to 0.64956, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 3/25\n129/129 [==============================] - 119s 923ms/step - loss: 1.1017 - acc: 0.7843 - val_loss: 1.5193 - val_acc: 0.6881\n\nEpoch 00003: val_acc improved from 0.64956 to 0.68813, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 4/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.7819 - acc: 0.8542 - val_loss: 1.5125 - val_acc: 0.6760\n\nEpoch 00004: val_acc did not improve from 0.68813\nEpoch 5/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.5674 - acc: 0.8993 - val_loss: 1.5473 - val_acc: 0.6851\n\nEpoch 00005: val_acc did not improve from 0.68813\nEpoch 6/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.4050 - acc: 0.9320 - val_loss: 1.3980 - val_acc: 0.6978\n\nEpoch 00006: val_acc improved from 0.68813 to 0.69784, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 7/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.3611 - acc: 0.9355 - val_loss: 1.6813 - val_acc: 0.6735\n\nEpoch 00007: val_acc did not improve from 0.69784\nEpoch 8/25\n129/129 [==============================] - 119s 921ms/step - loss: 0.2546 - acc: 0.9530 - val_loss: 1.3830 - val_acc: 0.7021\n\nEpoch 00008: val_acc improved from 0.69784 to 0.70210, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 9/25\n129/129 [==============================] - 119s 925ms/step - loss: 0.2246 - acc: 0.9612 - val_loss: 1.4073 - val_acc: 0.7012\n\nEpoch 00009: val_acc did not improve from 0.70210\nEpoch 10/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.1864 - acc: 0.9717 - val_loss: 1.3713 - val_acc: 0.6997\n\nEpoch 00010: val_acc did not improve from 0.70210\nEpoch 11/25\n129/129 [==============================] - 119s 921ms/step - loss: 0.1495 - acc: 0.9753 - val_loss: 1.4367 - val_acc: 0.6942\n\nEpoch 00011: val_acc did not improve from 0.70210\nEpoch 12/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.1419 - acc: 0.9770 - val_loss: 1.3873 - val_acc: 0.6942\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00012: val_acc did not improve from 0.70210\nEpoch 13/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.0981 - acc: 0.9864 - val_loss: 1.2111 - val_acc: 0.7303\n\nEpoch 00013: val_acc improved from 0.70210 to 0.73034, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 14/25\n129/129 [==============================] - 120s 929ms/step - loss: 0.0551 - acc: 0.9944 - val_loss: 1.1839 - val_acc: 0.7376\n\nEpoch 00014: val_acc improved from 0.73034 to 0.73763, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 15/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0516 - acc: 0.9947 - val_loss: 1.1981 - val_acc: 0.7425\n\nEpoch 00015: val_acc improved from 0.73763 to 0.74248, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 16/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0484 - acc: 0.9952 - val_loss: 1.2524 - val_acc: 0.7358\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00016: val_acc did not improve from 0.74248\nEpoch 17/25\n129/129 [==============================] - 119s 920ms/step - loss: 0.0400 - acc: 0.9957 - val_loss: 1.1632 - val_acc: 0.7446\n\nEpoch 00017: val_acc improved from 0.74248 to 0.74461, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 18/25\n129/129 [==============================] - 119s 926ms/step - loss: 0.0313 - acc: 0.9973 - val_loss: 1.1767 - val_acc: 0.7446\n\nEpoch 00018: val_acc did not improve from 0.74461\nEpoch 19/25\n129/129 [==============================] - 119s 920ms/step - loss: 0.0287 - acc: 0.9964 - val_loss: 1.1572 - val_acc: 0.7501\n\nEpoch 00019: val_acc improved from 0.74461 to 0.75008, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 20/25\n129/129 [==============================] - 119s 926ms/step - loss: 0.0251 - acc: 0.9976 - val_loss: 1.1504 - val_acc: 0.7510\n\nEpoch 00020: val_acc improved from 0.75008 to 0.75099, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 21/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.0248 - acc: 0.9978 - val_loss: 1.1759 - val_acc: 0.7528\n\nEpoch 00021: val_acc improved from 0.75099 to 0.75281, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 22/25\n129/129 [==============================] - 121s 937ms/step - loss: 0.0202 - acc: 0.9985 - val_loss: 1.1801 - val_acc: 0.7519\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00022: val_acc did not improve from 0.75281\nEpoch 23/25\n129/129 [==============================] - 119s 922ms/step - loss: 0.0208 - acc: 0.9993 - val_loss: 1.1365 - val_acc: 0.7571\n\nEpoch 00023: val_acc improved from 0.75281 to 0.75706, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 24/25\n129/129 [==============================] - 119s 923ms/step - loss: 0.0197 - acc: 0.9983 - val_loss: 1.1365 - val_acc: 0.7647\n\nEpoch 00024: val_acc improved from 0.75706 to 0.76465, saving model to inception_resnet_v2_relu_softmax_10.h5\nEpoch 25/25\n129/129 [==============================] - 119s 924ms/step - loss: 0.0188 - acc: 0.9986 - val_loss: 1.2029 - val_acc: 0.7461\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00025: val_acc did not improve from 0.76465\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":29,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.1206932214427652, 0.7601548857519109]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Xception Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Xception\nfrom keras.applications.xception import Xception\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = Xception(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\")\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"xception.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=20,\n                              callbacks=[reducel, checkpoint])","execution_count":19,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n129/129 [==============================] - 77s 599ms/step - loss: 1.9983 - acc: 0.5819 - val_loss: 1.5942 - val_acc: 0.5940\n\nEpoch 00001: val_acc improved from -inf to 0.59399, saving model to xception.h5\nEpoch 2/20\n129/129 [==============================] - 60s 468ms/step - loss: 0.4172 - acc: 0.9139 - val_loss: 1.4777 - val_acc: 0.6213\n\nEpoch 00002: val_acc improved from 0.59399 to 0.62132, saving model to xception.h5\nEpoch 3/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.2132 - acc: 0.9555 - val_loss: 1.4880 - val_acc: 0.6383\n\nEpoch 00003: val_acc improved from 0.62132 to 0.63832, saving model to xception.h5\nEpoch 4/20\n129/129 [==============================] - 61s 472ms/step - loss: 0.1370 - acc: 0.9734 - val_loss: 1.4914 - val_acc: 0.6408\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00004: val_acc improved from 0.63832 to 0.64075, saving model to xception.h5\nEpoch 5/20\n129/129 [==============================] - 61s 475ms/step - loss: 0.0971 - acc: 0.9838 - val_loss: 1.4381 - val_acc: 0.6408\n\nEpoch 00005: val_acc improved from 0.64075 to 0.64075, saving model to xception.h5\nEpoch 6/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0545 - acc: 0.9935 - val_loss: 1.4487 - val_acc: 0.6435\n\nEpoch 00006: val_acc improved from 0.64075 to 0.64349, saving model to xception.h5\nEpoch 7/20\n129/129 [==============================] - 61s 470ms/step - loss: 0.0447 - acc: 0.9947 - val_loss: 1.5057 - val_acc: 0.6359\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00007: val_acc did not improve from 0.64349\nEpoch 8/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0431 - acc: 0.9937 - val_loss: 1.4283 - val_acc: 0.6575\n\nEpoch 00008: val_acc improved from 0.64349 to 0.65746, saving model to xception.h5\nEpoch 9/20\n129/129 [==============================] - 61s 476ms/step - loss: 0.0317 - acc: 0.9964 - val_loss: 1.4342 - val_acc: 0.6508\n\nEpoch 00009: val_acc did not improve from 0.65746\nEpoch 10/20\n129/129 [==============================] - 61s 471ms/step - loss: 0.0294 - acc: 0.9965 - val_loss: 1.4855 - val_acc: 0.6471\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 00010: val_acc did not improve from 0.65746\nEpoch 11/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0211 - acc: 0.9978 - val_loss: 1.4636 - val_acc: 0.6502\n\nEpoch 00011: val_acc did not improve from 0.65746\nEpoch 12/20\n129/129 [==============================] - 61s 469ms/step - loss: 0.0213 - acc: 0.9981 - val_loss: 1.4402 - val_acc: 0.6529\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 00012: val_acc did not improve from 0.65746\nEpoch 13/20\n129/129 [==============================] - 61s 470ms/step - loss: 0.0197 - acc: 0.9981 - val_loss: 1.4362 - val_acc: 0.6490\n\nEpoch 00013: val_acc did not improve from 0.65746\nEpoch 14/20\n129/129 [==============================] - 62s 482ms/step - loss: 0.0179 - acc: 0.9985 - val_loss: 1.4435 - val_acc: 0.6505\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\nEpoch 00014: val_acc did not improve from 0.65746\nEpoch 15/20\n129/129 [==============================] - 60s 466ms/step - loss: 0.0171 - acc: 0.9988 - val_loss: 1.4784 - val_acc: 0.6444\n\nEpoch 00015: val_acc did not improve from 0.65746\nEpoch 16/20\n129/129 [==============================] - 60s 467ms/step - loss: 0.0157 - acc: 0.9993 - val_loss: 1.4471 - val_acc: 0.6468\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\nEpoch 00016: val_acc did not improve from 0.65746\nEpoch 17/20\n129/129 [==============================] - 60s 469ms/step - loss: 0.0157 - acc: 0.9988 - val_loss: 1.4730 - val_acc: 0.6496\n\nEpoch 00017: val_acc did not improve from 0.65746\nEpoch 18/20\n129/129 [==============================] - 61s 471ms/step - loss: 0.0158 - acc: 0.9993 - val_loss: 1.4442 - val_acc: 0.6572\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n\nEpoch 00018: val_acc did not improve from 0.65746\nEpoch 19/20\n129/129 [==============================] - 61s 474ms/step - loss: 0.0158 - acc: 0.9990 - val_loss: 1.4655 - val_acc: 0.6517\n\nEpoch 00019: val_acc did not improve from 0.65746\nEpoch 20/20\n129/129 [==============================] - 60s 467ms/step - loss: 0.0149 - acc: 0.9990 - val_loss: 1.4345 - val_acc: 0.6544\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n\nEpoch 00020: val_acc did not improve from 0.65746\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":20,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.3929535331183744, 0.6614531926608793]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## ResNet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"## ResNet50\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = ResNet50(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":46,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train resnet50 model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"ResNet50.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=10,\n                              callbacks=[reducel, checkpoint])","execution_count":47,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n129/129 [==============================] - 150s 1s/step - loss: 3.9181 - acc: 0.1622 - val_loss: 3.3425 - val_acc: 0.2305\n\nEpoch 00001: val_acc improved from -inf to 0.23049, saving model to ResNet50.h5\nEpoch 2/10\n129/129 [==============================] - 47s 364ms/step - loss: 2.0939 - acc: 0.4569 - val_loss: 2.3535 - val_acc: 0.3863\n\nEpoch 00002: val_acc improved from 0.23049 to 0.38627, saving model to ResNet50.h5\nEpoch 3/10\n129/129 [==============================] - 47s 366ms/step - loss: 1.3109 - acc: 0.6444 - val_loss: 2.2176 - val_acc: 0.4197\n\nEpoch 00003: val_acc improved from 0.38627 to 0.41968, saving model to ResNet50.h5\nEpoch 4/10\n129/129 [==============================] - 46s 359ms/step - loss: 0.7823 - acc: 0.7886 - val_loss: 2.1262 - val_acc: 0.4388\n\nEpoch 00004: val_acc improved from 0.41968 to 0.43881, saving model to ResNet50.h5\nEpoch 5/10\n129/129 [==============================] - 46s 357ms/step - loss: 0.5053 - acc: 0.8760 - val_loss: 2.1993 - val_acc: 0.4303\n\nEpoch 00005: val_acc did not improve from 0.43881\nEpoch 6/10\n129/129 [==============================] - 46s 360ms/step - loss: 0.3262 - acc: 0.9206 - val_loss: 2.3725 - val_acc: 0.4212\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00006: val_acc did not improve from 0.43881\nEpoch 7/10\n129/129 [==============================] - 47s 361ms/step - loss: 0.1779 - acc: 0.9623 - val_loss: 1.8675 - val_acc: 0.5178\n\nEpoch 00007: val_acc improved from 0.43881 to 0.51776, saving model to ResNet50.h5\nEpoch 8/10\n129/129 [==============================] - 46s 360ms/step - loss: 0.1094 - acc: 0.9787 - val_loss: 1.8810 - val_acc: 0.5153\n\nEpoch 00008: val_acc did not improve from 0.51776\nEpoch 9/10\n129/129 [==============================] - 47s 365ms/step - loss: 0.0801 - acc: 0.9867 - val_loss: 1.8601 - val_acc: 0.5272\n\nEpoch 00009: val_acc improved from 0.51776 to 0.52718, saving model to ResNet50.h5\nEpoch 10/10\n129/129 [==============================] - 46s 360ms/step - loss: 0.0626 - acc: 0.9898 - val_loss: 1.8861 - val_acc: 0.5178\n\nEpoch 00010: val_acc did not improve from 0.52718\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":48,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.822889103631334, 0.5343557816052815]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"## InceptionV3\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.models import Sequential, Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Concatenate, BatchNormalization, Flatten, Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n\nIMG_SIZE = (IMAGE_SIZE, IMAGE_SIZE)\nIN_SHAPE = (*IMG_SIZE, 3)\n\nconv_base = InceptionV3(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(layers.Dense(1024, activation = \"relu\"))\nmodel.add(layers.Dense(120, activation = \"softmax\"))\n\n# conv_base.summary()\nconv_base.Trainable=True\n\nfrom keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=[\"accuracy\"])\n# model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":36,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87916544/87910968 [==============================] - 2s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train inception_resnet model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nh5_path = \"InceptionV3.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1,restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr = 0.000000001)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                              validation_data=val_gen, validation_steps=val_steps,\n                              epochs=10,\n                              callbacks=[reducel, checkpoint])","execution_count":37,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n129/129 [==============================] - 128s 989ms/step - loss: 3.9884 - acc: 0.1672 - val_loss: 2.8011 - val_acc: 0.3371\n\nEpoch 00001: val_acc improved from -inf to 0.33708, saving model to InceptionV3.h5\nEpoch 2/10\n129/129 [==============================] - 41s 314ms/step - loss: 2.1775 - acc: 0.4565 - val_loss: 2.0992 - val_acc: 0.4446\n\nEpoch 00002: val_acc improved from 0.33708 to 0.44458, saving model to InceptionV3.h5\nEpoch 3/10\n129/129 [==============================] - 41s 321ms/step - loss: 1.4280 - acc: 0.6071 - val_loss: 1.9280 - val_acc: 0.4719\n\nEpoch 00003: val_acc improved from 0.44458 to 0.47191, saving model to InceptionV3.h5\nEpoch 4/10\n129/129 [==============================] - 41s 318ms/step - loss: 0.9760 - acc: 0.7259 - val_loss: 1.7866 - val_acc: 0.5141\n\nEpoch 00004: val_acc improved from 0.47191 to 0.51412, saving model to InceptionV3.h5\nEpoch 5/10\n129/129 [==============================] - 43s 330ms/step - loss: 0.6936 - acc: 0.8045 - val_loss: 1.9628 - val_acc: 0.4974\n\nEpoch 00005: val_acc did not improve from 0.51412\nEpoch 6/10\n129/129 [==============================] - 41s 315ms/step - loss: 0.5228 - acc: 0.8518 - val_loss: 1.9430 - val_acc: 0.5080\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 00006: val_acc did not improve from 0.51412\nEpoch 7/10\n129/129 [==============================] - 40s 314ms/step - loss: 0.3247 - acc: 0.9136 - val_loss: 1.6171 - val_acc: 0.5739\n\nEpoch 00007: val_acc improved from 0.51412 to 0.57394, saving model to InceptionV3.h5\nEpoch 8/10\n129/129 [==============================] - 42s 327ms/step - loss: 0.1992 - acc: 0.9548 - val_loss: 1.6619 - val_acc: 0.5627\n\nEpoch 00008: val_acc did not improve from 0.57394\nEpoch 9/10\n129/129 [==============================] - 40s 314ms/step - loss: 0.1665 - acc: 0.9602 - val_loss: 1.6466 - val_acc: 0.5736\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 00009: val_acc did not improve from 0.57394\nEpoch 10/10\n129/129 [==============================] - 41s 318ms/step - loss: 0.1250 - acc: 0.9744 - val_loss: 1.6482 - val_acc: 0.5834\n\nEpoch 00010: val_acc improved from 0.57394 to 0.58336, saving model to InceptionV3.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy metric 1\nfrom keras.metrics import categorical_accuracy\nresult = model.evaluate_generator(test_gen,steps = test_steps)\nprint(model.metrics_names)\nprint(result)","execution_count":38,"outputs":[{"output_type":"stream","text":"['loss', 'acc']\n[1.5274156829543486, 0.6010933110259785]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}